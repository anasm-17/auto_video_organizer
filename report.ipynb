{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import gc\n",
    "import speech_recognition as sr\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "#### [1. Preprocessing video and audio](#1.0)\n",
    "\n",
    "- [1.1. convert video to audio (wav) files](#1.1)\n",
    "- [1.2. convert audio to text [1] [2]](#1.2)\n",
    "\n",
    "#### [2. Performing text similarity matching](#2.0)\n",
    "\n",
    "- [2.1 Read ipynb notebooks](#2.1)\n",
    "- [2.2 Preprocess text](#2.2)\n",
    "- [2.3 Performing text similarity](#2.3)\n",
    "\n",
    "#### [3. Results](#3.0)\n",
    "#### [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing video and audio <a class=\"anchor\" id=\"1.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 convert video to audio (wav) files <a class=\"anchor\" id=1.1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using `lecture 3 - floating points` and `lecture 5 - neural networks` from Mike Gelbart's [Supervised Learning II playlist](https://www.youtube.com/playlist?list=PLWmXHcz_53Q3KLISD8jydKjz41b9iqERC)\n",
    "\n",
    "Place video files in mp4 format to `data/video/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 load video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_1 = AudioSegment.from_file(\"data/video/20200120-102822-008.mp4\")\n",
    "vid_2 = AudioSegment.from_file(\"data/video/20200127-102817-008.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we will use this dictionary for the results section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vid_1': '20200120-102822-008.mp4', 'vid_2': '20200127-102817-008.mp4'}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_dict = {f\"vid_{i+1}\" : os.listdir(\"data/video\")[i] for i in range(len(os.listdir(\"data/video\")))}\n",
    "videos_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Providing the URLs for readers to view the videos as you do not have access to the video dump available for UBC Master of Data Science students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20200120-102822-008.mp4': 'https://youtu.be/kNxLxwIduko',\n",
       " '20200127-102817-008.mp4': 'https://youtu.be/1wYHXT-Ee5U'}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_urls = {\"20200120-102822-008.mp4\": \"https://youtu.be/kNxLxwIduko\",\n",
    "              \"20200127-102817-008.mp4\": \"https://youtu.be/1wYHXT-Ee5U\"}\n",
    "video_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Convert video to audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to convert to wav format for speech recognition. I will not be performing any audio preprocessing as I'll be working on the baseline model, with minimal effort. If the results are not favorable I may preprocess the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='data/audio/vid_2.wav'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_1.export(\"data/audio/vid_1.wav\", format=\"wav\")\n",
    "vid_2.export(\"data/audio/vid_2.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vid_1, vid_2\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convert audio to text [1] [2] <a class=\"anchor\" id=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Chunk audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to chunk audio files as I am using Google's free speech recognition API, which only processes 5 seconds worth of information at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_chunker(audio_files_dir, op_dir, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    Takes in list of audio files\n",
    "    directories and prepares \n",
    "    audio segment to process into\n",
    "    audio chunks. Output of chunk\n",
    "    will be under subdirectory of\n",
    "    video title under op_dir/vid_n\n",
    "    where n is count of video.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    audio_files_dir : list\n",
    "        list of audio files to\n",
    "        create chunks of.\n",
    "    \n",
    "    op_dir : str\n",
    "        output directory to \n",
    "        store chunks.\n",
    "    \n",
    "    chunk_size : int\n",
    "        size of audio chunks in ms,\n",
    "        default 5000 as Google API\n",
    "        supports 5 seconds.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> audio_files = [\"data/audio/vid_1.wav\", \"data/audio/vid_2.wav\"]\n",
    "    >>> audio_chunker(audio_files, \"data/audio_chunks/\")\n",
    "    \"\"\"\n",
    "    audio_segs = [AudioSegment.from_wav(audio_files[0]),\n",
    "                  AudioSegment.from_wav(audio_files[1])]\n",
    "    \n",
    "    for count, audio_seg in enumerate(audio_segs):\n",
    "        \n",
    "        print(f\"Prcoessing audio file {count+1}/{len(audio_segs)}...\")\n",
    "        len_audio = len(audio_seg) # length of audio segment\n",
    "        n_chunks = len_audio//chunk_size # calculate chink size\n",
    "        \n",
    "        storage_dir = f\"{op_dir}vid_{count+1}/\"\n",
    "        os.mkdir(storage_dir) # create storage dir\n",
    "        \n",
    "        # create chunks and export to directory\n",
    "        for i in range(n_chunks):\n",
    "            \n",
    "            if i == 0:\n",
    "                start = 0\n",
    "                end = chunk_size\n",
    "            else:\n",
    "                start = end\n",
    "                end = start + chunk_size\n",
    "\n",
    "            chunk = audio_seg[start:end]\n",
    "            \n",
    "            # write chunk to `storage_dir`\n",
    "            chunk.export(f\"{storage_dir}chunk_{i}.wav\", format=\"wav\")\n",
    "    \n",
    "    print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prcoessing audio file 1/2...\n",
      "Prcoessing audio file 2/2...\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "audio_files = [\"data/audio/vid_1.wav\", \"data/audio/vid_2.wav\"]\n",
    "audio_chunker(audio_files, \"data/audio_chunks/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Speech recognition and saving to text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the speech recognition part, I will extract approximately 10k characters to build a baseline model and to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_chunk_to_text(audio_chunks_dir, op_dir, max_text_len=10000):\n",
    "    \"\"\"\n",
    "    Takes in audio chunks directory\n",
    "    and performs speech recognition\n",
    "    on each chunk and stores text\n",
    "    to vid_n.txt where n is the \n",
    "    count of the video.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    audio_chunks_dir : str\n",
    "        parent directory where audio \n",
    "        of where audio chunks for \n",
    "        video_n's are located.\n",
    "    \n",
    "    op_dir : str\n",
    "        text storage directory,\n",
    "        will be in the form video_n.txt,\n",
    "        where n is the count of video.\n",
    "        \n",
    "    max_text_len : \n",
    "        will stop the speech recognition\n",
    "        after if exceeds max_character length.\n",
    "        \n",
    "    Returns:\n",
    "    ---------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # set up speech recognizer class\n",
    "    r = sr.Recognizer()\n",
    "    \n",
    "    # audio chunk files\n",
    "    audio_chunk_videos = os.listdir(audio_chunks_dir)\n",
    "    \n",
    "    for count, video in enumerate(audio_chunk_videos):\n",
    "        # load audio chunks\n",
    "        audio_chunks = os.listdir(audio_chunks_dir + video)\n",
    "        print(f\"Processing video: {count+1}/{len(audio_chunk_videos)}\")\n",
    "        print(\"---\"*12)\n",
    "        output_text = \"\"\n",
    "        step_counter = 0\n",
    "        \n",
    "        # perform speech recognition\n",
    "        # on each chunk\n",
    "        for i in range(len(audio_chunk_files)):\n",
    "            if i%5 == 0:\n",
    "                print(f\"Step {step_counter+1}\")\n",
    "                print(f\"Char length: {len(output_text)}\")\n",
    "                print(f\"Processing chunk: {i+1}/{len(audio_chunks)}...\")\n",
    "                step_counter += 1\n",
    "            \n",
    "            # read chunk\n",
    "            aud_file = f\"{audio_chunks_dir}{video}/chunk_{i}.wav\"\n",
    "            \n",
    "            with sr.AudioFile(aud_file) as source:\n",
    "                # adjust for ambient noise\n",
    "                r.adjust_for_ambient_noise(source)\n",
    "                \n",
    "                # get the text from audio from chunk\n",
    "                audio = r.listen(source)\n",
    "            \n",
    "            # try to perform speech recognition\n",
    "            try:\n",
    "                # if succeeds, append text to\n",
    "                # output text\n",
    "                text = r.recognize_google(audio)\n",
    "                output_text += text + \" \"\n",
    "\n",
    "            except Exception as e:\n",
    "                # if fails, pass to the\n",
    "                # next loop/chunk\n",
    "                pass\n",
    "            \n",
    "            # early stopping, as I do not\n",
    "            # wish to process the entire\n",
    "            # set of chunks. If output text\n",
    "            # length reached, break.\n",
    "            if len(output_text) > max_text_len:\n",
    "                break\n",
    "\n",
    "            time.sleep(0.5) # to be nice to google's free API\n",
    "        \n",
    "        # write output to text file\n",
    "        text_file = open(f\"{op_dir}transcribed_{video}.txt\", \"w+\")\n",
    "        text_file.write(output_text)\n",
    "        text_file.close()\n",
    "        print(f\"Process complete! {i+1}/{len(audio_chunks)} converted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[SKIP LENGHTHY OUTPUT](#skip_output)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: 1/2\n",
      "------------------------------------\n",
      "Step 1\n",
      "Char length: 0\n",
      "Processing chunk: 1/1018...\n",
      "Step 2\n",
      "Char length: 195\n",
      "Processing chunk: 6/1018...\n",
      "Step 3\n",
      "Char length: 351\n",
      "Processing chunk: 11/1018...\n",
      "Step 4\n",
      "Char length: 486\n",
      "Processing chunk: 16/1018...\n",
      "Step 5\n",
      "Char length: 641\n",
      "Processing chunk: 21/1018...\n",
      "Step 6\n",
      "Char length: 753\n",
      "Processing chunk: 26/1018...\n",
      "Step 7\n",
      "Char length: 887\n",
      "Processing chunk: 31/1018...\n",
      "Step 8\n",
      "Char length: 1052\n",
      "Processing chunk: 36/1018...\n",
      "Step 9\n",
      "Char length: 1168\n",
      "Processing chunk: 41/1018...\n",
      "Step 10\n",
      "Char length: 1351\n",
      "Processing chunk: 46/1018...\n",
      "Step 11\n",
      "Char length: 1483\n",
      "Processing chunk: 51/1018...\n",
      "Step 12\n",
      "Char length: 1683\n",
      "Processing chunk: 56/1018...\n",
      "Step 13\n",
      "Char length: 1841\n",
      "Processing chunk: 61/1018...\n",
      "Step 14\n",
      "Char length: 2023\n",
      "Processing chunk: 66/1018...\n",
      "Step 15\n",
      "Char length: 2182\n",
      "Processing chunk: 71/1018...\n",
      "Step 16\n",
      "Char length: 2381\n",
      "Processing chunk: 76/1018...\n",
      "Step 17\n",
      "Char length: 2596\n",
      "Processing chunk: 81/1018...\n",
      "Step 18\n",
      "Char length: 2750\n",
      "Processing chunk: 86/1018...\n",
      "Step 19\n",
      "Char length: 3014\n",
      "Processing chunk: 91/1018...\n",
      "Step 20\n",
      "Char length: 3171\n",
      "Processing chunk: 96/1018...\n",
      "Step 21\n",
      "Char length: 3357\n",
      "Processing chunk: 101/1018...\n",
      "Step 22\n",
      "Char length: 3545\n",
      "Processing chunk: 106/1018...\n",
      "Step 23\n",
      "Char length: 3771\n",
      "Processing chunk: 111/1018...\n",
      "Step 24\n",
      "Char length: 3840\n",
      "Processing chunk: 116/1018...\n",
      "Step 25\n",
      "Char length: 3992\n",
      "Processing chunk: 121/1018...\n",
      "Step 26\n",
      "Char length: 4167\n",
      "Processing chunk: 126/1018...\n",
      "Step 27\n",
      "Char length: 4264\n",
      "Processing chunk: 131/1018...\n",
      "Step 28\n",
      "Char length: 4467\n",
      "Processing chunk: 136/1018...\n",
      "Step 29\n",
      "Char length: 4534\n",
      "Processing chunk: 141/1018...\n",
      "Step 30\n",
      "Char length: 4677\n",
      "Processing chunk: 146/1018...\n",
      "Step 31\n",
      "Char length: 4830\n",
      "Processing chunk: 151/1018...\n",
      "Step 32\n",
      "Char length: 4895\n",
      "Processing chunk: 156/1018...\n",
      "Step 33\n",
      "Char length: 5052\n",
      "Processing chunk: 161/1018...\n",
      "Step 34\n",
      "Char length: 5301\n",
      "Processing chunk: 166/1018...\n",
      "Step 35\n",
      "Char length: 5551\n",
      "Processing chunk: 171/1018...\n",
      "Step 36\n",
      "Char length: 5690\n",
      "Processing chunk: 176/1018...\n",
      "Step 37\n",
      "Char length: 5944\n",
      "Processing chunk: 181/1018...\n",
      "Step 38\n",
      "Char length: 6200\n",
      "Processing chunk: 186/1018...\n",
      "Step 39\n",
      "Char length: 6277\n",
      "Processing chunk: 191/1018...\n",
      "Step 40\n",
      "Char length: 6446\n",
      "Processing chunk: 196/1018...\n",
      "Step 41\n",
      "Char length: 6606\n",
      "Processing chunk: 201/1018...\n",
      "Step 42\n",
      "Char length: 6790\n",
      "Processing chunk: 206/1018...\n",
      "Step 43\n",
      "Char length: 6919\n",
      "Processing chunk: 211/1018...\n",
      "Step 44\n",
      "Char length: 7133\n",
      "Processing chunk: 216/1018...\n",
      "Step 45\n",
      "Char length: 7336\n",
      "Processing chunk: 221/1018...\n",
      "Step 46\n",
      "Char length: 7669\n",
      "Processing chunk: 226/1018...\n",
      "Step 47\n",
      "Char length: 7975\n",
      "Processing chunk: 231/1018...\n",
      "Step 48\n",
      "Char length: 8124\n",
      "Processing chunk: 236/1018...\n",
      "Step 49\n",
      "Char length: 8345\n",
      "Processing chunk: 241/1018...\n",
      "Step 50\n",
      "Char length: 8529\n",
      "Processing chunk: 246/1018...\n",
      "Step 51\n",
      "Char length: 8697\n",
      "Processing chunk: 251/1018...\n",
      "Step 52\n",
      "Char length: 8946\n",
      "Processing chunk: 256/1018...\n",
      "Step 53\n",
      "Char length: 9122\n",
      "Processing chunk: 261/1018...\n",
      "Step 54\n",
      "Char length: 9272\n",
      "Processing chunk: 266/1018...\n",
      "Step 55\n",
      "Char length: 9513\n",
      "Processing chunk: 271/1018...\n",
      "Step 56\n",
      "Char length: 9757\n",
      "Processing chunk: 276/1018...\n",
      "Step 57\n",
      "Char length: 9843\n",
      "Processing chunk: 281/1018...\n",
      "Step 58\n",
      "Char length: 9999\n",
      "Processing chunk: 286/1018...\n",
      "Process complete! 286/1018 converted.\n",
      "Processing video: 2/2\n",
      "------------------------------------\n",
      "Step 1\n",
      "Char length: 0\n",
      "Processing chunk: 1/1018...\n",
      "Step 2\n",
      "Char length: 19\n",
      "Processing chunk: 6/1018...\n",
      "Step 3\n",
      "Char length: 124\n",
      "Processing chunk: 11/1018...\n",
      "Step 4\n",
      "Char length: 375\n",
      "Processing chunk: 16/1018...\n",
      "Step 5\n",
      "Char length: 596\n",
      "Processing chunk: 21/1018...\n",
      "Step 6\n",
      "Char length: 799\n",
      "Processing chunk: 26/1018...\n",
      "Step 7\n",
      "Char length: 1041\n",
      "Processing chunk: 31/1018...\n",
      "Step 8\n",
      "Char length: 1298\n",
      "Processing chunk: 36/1018...\n",
      "Step 9\n",
      "Char length: 1569\n",
      "Processing chunk: 41/1018...\n",
      "Step 10\n",
      "Char length: 1789\n",
      "Processing chunk: 46/1018...\n",
      "Step 11\n",
      "Char length: 1864\n",
      "Processing chunk: 51/1018...\n",
      "Step 12\n",
      "Char length: 2095\n",
      "Processing chunk: 56/1018...\n",
      "Step 13\n",
      "Char length: 2248\n",
      "Processing chunk: 61/1018...\n",
      "Step 14\n",
      "Char length: 2488\n",
      "Processing chunk: 66/1018...\n",
      "Step 15\n",
      "Char length: 2608\n",
      "Processing chunk: 71/1018...\n",
      "Step 16\n",
      "Char length: 2783\n",
      "Processing chunk: 76/1018...\n",
      "Step 17\n",
      "Char length: 3026\n",
      "Processing chunk: 81/1018...\n",
      "Step 18\n",
      "Char length: 3226\n",
      "Processing chunk: 86/1018...\n",
      "Step 19\n",
      "Char length: 3452\n",
      "Processing chunk: 91/1018...\n",
      "Step 20\n",
      "Char length: 3695\n",
      "Processing chunk: 96/1018...\n",
      "Step 21\n",
      "Char length: 3999\n",
      "Processing chunk: 101/1018...\n",
      "Step 22\n",
      "Char length: 4095\n",
      "Processing chunk: 106/1018...\n",
      "Step 23\n",
      "Char length: 4122\n",
      "Processing chunk: 111/1018...\n",
      "Step 24\n",
      "Char length: 4285\n",
      "Processing chunk: 116/1018...\n",
      "Step 25\n",
      "Char length: 4397\n",
      "Processing chunk: 121/1018...\n",
      "Step 26\n",
      "Char length: 4568\n",
      "Processing chunk: 126/1018...\n",
      "Step 27\n",
      "Char length: 4721\n",
      "Processing chunk: 131/1018...\n",
      "Step 28\n",
      "Char length: 4875\n",
      "Processing chunk: 136/1018...\n",
      "Step 29\n",
      "Char length: 4999\n",
      "Processing chunk: 141/1018...\n",
      "Step 30\n",
      "Char length: 5163\n",
      "Processing chunk: 146/1018...\n",
      "Step 31\n",
      "Char length: 5329\n",
      "Processing chunk: 151/1018...\n",
      "Step 32\n",
      "Char length: 5593\n",
      "Processing chunk: 156/1018...\n",
      "Step 33\n",
      "Char length: 5895\n",
      "Processing chunk: 161/1018...\n",
      "Step 34\n",
      "Char length: 6125\n",
      "Processing chunk: 166/1018...\n",
      "Step 35\n",
      "Char length: 6287\n",
      "Processing chunk: 171/1018...\n",
      "Step 36\n",
      "Char length: 6478\n",
      "Processing chunk: 176/1018...\n",
      "Step 37\n",
      "Char length: 6730\n",
      "Processing chunk: 181/1018...\n",
      "Step 38\n",
      "Char length: 6923\n",
      "Processing chunk: 186/1018...\n",
      "Step 39\n",
      "Char length: 7163\n",
      "Processing chunk: 191/1018...\n",
      "Step 40\n",
      "Char length: 7305\n",
      "Processing chunk: 196/1018...\n",
      "Step 41\n",
      "Char length: 7392\n",
      "Processing chunk: 201/1018...\n",
      "Step 42\n",
      "Char length: 7471\n",
      "Processing chunk: 206/1018...\n",
      "Step 43\n",
      "Char length: 7594\n",
      "Processing chunk: 211/1018...\n",
      "Step 44\n",
      "Char length: 7594\n",
      "Processing chunk: 216/1018...\n",
      "Step 45\n",
      "Char length: 7594\n",
      "Processing chunk: 221/1018...\n",
      "Step 46\n",
      "Char length: 7594\n",
      "Processing chunk: 226/1018...\n",
      "Step 47\n",
      "Char length: 7594\n",
      "Processing chunk: 231/1018...\n",
      "Step 48\n",
      "Char length: 7594\n",
      "Processing chunk: 236/1018...\n",
      "Step 49\n",
      "Char length: 7594\n",
      "Processing chunk: 241/1018...\n",
      "Step 50\n",
      "Char length: 7594\n",
      "Processing chunk: 246/1018...\n",
      "Step 51\n",
      "Char length: 7594\n",
      "Processing chunk: 251/1018...\n",
      "Step 52\n",
      "Char length: 7594\n",
      "Processing chunk: 256/1018...\n",
      "Step 53\n",
      "Char length: 7744\n",
      "Processing chunk: 261/1018...\n",
      "Step 54\n",
      "Char length: 7959\n",
      "Processing chunk: 266/1018...\n",
      "Step 55\n",
      "Char length: 8176\n",
      "Processing chunk: 271/1018...\n",
      "Step 56\n",
      "Char length: 8398\n",
      "Processing chunk: 276/1018...\n",
      "Step 57\n",
      "Char length: 8634\n",
      "Processing chunk: 281/1018...\n",
      "Step 58\n",
      "Char length: 8826\n",
      "Processing chunk: 286/1018...\n",
      "Step 59\n",
      "Char length: 9007\n",
      "Processing chunk: 291/1018...\n",
      "Step 60\n",
      "Char length: 9201\n",
      "Processing chunk: 296/1018...\n",
      "Step 61\n",
      "Char length: 9317\n",
      "Processing chunk: 301/1018...\n",
      "Step 62\n",
      "Char length: 9450\n",
      "Processing chunk: 306/1018...\n",
      "Step 63\n",
      "Char length: 9573\n",
      "Processing chunk: 311/1018...\n",
      "Step 64\n",
      "Char length: 9613\n",
      "Processing chunk: 316/1018...\n",
      "Step 65\n",
      "Char length: 9613\n",
      "Processing chunk: 321/1018...\n",
      "Step 66\n",
      "Char length: 9772\n",
      "Processing chunk: 326/1018...\n",
      "Step 67\n",
      "Char length: 9952\n",
      "Processing chunk: 331/1018...\n",
      "Step 68\n",
      "Char length: 9952\n",
      "Processing chunk: 336/1018...\n",
      "Step 69\n",
      "Char length: 9952\n",
      "Processing chunk: 341/1018...\n",
      "Step 70\n",
      "Char length: 9952\n",
      "Processing chunk: 346/1018...\n",
      "Step 71\n",
      "Char length: 9952\n",
      "Processing chunk: 351/1018...\n",
      "Step 72\n",
      "Char length: 9952\n",
      "Processing chunk: 356/1018...\n",
      "Step 73\n",
      "Char length: 9952\n",
      "Processing chunk: 361/1018...\n",
      "Step 74\n",
      "Char length: 9952\n",
      "Processing chunk: 366/1018...\n",
      "Step 75\n",
      "Char length: 9952\n",
      "Processing chunk: 371/1018...\n",
      "Process complete! 374/1018 converted.\n"
     ]
    }
   ],
   "source": [
    "audio_chunk_to_text(\"data/audio_chunks/\", \"data/text/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"skip_output\"></a> Let's read in our trancribed text to see how well the speech recognition performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/text/transcribed_vid_1.txt\") as f:\n",
    "    vid_1_transcribed = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will read in the transcribed text for `vid_1` and look at the first 2000 characters to gage the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"let's get started are you good at the So today we're going to talk about things I have titled lecture which will hopefully make sense soon just sometimes set this whole week we talked about this posted in the read me I have deleted the set cuz the rest of the course was just too rushed and now I'm is really in I have posted fat heels a bit too loud can people hear me sweatshirt play nlg speak more of a standalone thing that needed to be somewhere so we're going to do it today here here's some motivating example so for those of you baby surprising but and when you run it it's not garbage number with you but you can try this in our or whatever Java whatever language you want you going to get here's another one I've added 1000 Best Buy ad such a don't believe me if you don't believe so this is somehow we haven't may not have noticed I'm so let's figure out what's going on here skip this way you can try it I'll say the funny story about this size using Matlab and I submitted people who make me laugh and I said hey thing is broken read this is floating Point issues okay the build-up to the all the way at the beginning of the story just for those which is assuming most of you have seen binary numbers before register super quick review so when we write a number like 52 what we mean by that 21610 s 530 same thing except the base is 2 in I want to write 13 I write it like this do you have a1021 4 1/8 1 + 4 + 8 + 13 Okay so X is an integer and we can read this one one as 1 * 2 to the 3 1/8 4 + 026 + online and if we had a 5 digit number of you have a 16 and 7 in you only need two symbols for the binary code call those symbols y-90 cuz we already have those symbols if you want we don't need another symbol for 2 cuz we can ready like this onesie in binary so we don't need another symbol we we can write everything we okay what about when it gets to like decimals so hold on the 1st numbers are actually stored in your computer in your memory as invited me talk a bit about that ear\""
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_1_transcribed[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That seems to have done a fairly good job without any preprocessing from my end! But you may notice some discontinuity, like in the first line \"let's get started are you good at the So today\", I believe this could be due to my abrupt chunking which does not seem like a good practice. How I am chunking now looks like:\n",
    "\n",
    "<img src=\"images/abrupt_chunking.PNG\">\n",
    "\n",
    "Here, with my abrupt chunking methodology, I the speech recognizer is unable to capture the word guidelines as it gets cut off. However, if I were to overlap my chunks:\n",
    "\n",
    "<img src=\"images/overlap_chunking.PNG\">\n",
    "\n",
    "Now I am able to capture guidelines. Probably by overlapping for 0.5 to 1 second, resulting in longer chunks but making the transcribed texts more coherent. For now I'll leave it be to build the baseline model to see if it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performing text similarity matching <a class=\"anchor\" id=\"2.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read ipynb notebooks <a class=\"anchor\" id=\"2.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Place [lecture3_floating-point.ipynb](https://github.com/UBC-MDS/DSCI_572_sup-learn-2/blob/master/lectures/lecture3_floating-point.ipynb) and [lecture5_neural-networks.ipynb](https://github.com/UBC-MDS/DSCI_572_sup-learn-2/blob/master/lectures/lecture5_neural-networks.ipynb) in `data/text/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Convert ipynb files to html for easier parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/text\\\\lecture3_floating-point.html'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"data/text/lec*\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [os.system(f\"jupyter nbconvert --to html {i}\") for i in glob.glob(\"data/text/lec*\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Read in text using beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_files = glob.glob(\"data/text/*.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipynb_html_text_parser(html_files_list):\n",
    "    \"\"\"\n",
    "    Parses text from jupyter notebook,\n",
    "    HTML files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    html_files : list\n",
    "        list of relative paths\n",
    "        to the html files.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    text_dict : dictionary\n",
    "        A dictionary of text\n",
    "        where keys are lecture\n",
    "        names and values are \n",
    "        respective texts.\n",
    "    \n",
    "    Example:\n",
    "    ---------\n",
    "    >>> html_files = glob.glob(\"data/text/*.html\")\n",
    "    >>> lecture_texts = ipynb_text_parser(html_files)\n",
    "    \"\"\"\n",
    "    lec_list = []\n",
    "    joined_text_list = []\n",
    "    for file in html_files_list:\n",
    "        # read in file using beautiful soup\n",
    "        soup = BeautifulSoup(open(file), \"html.parser\")\n",
    "        \n",
    "        # get all the text and join\n",
    "        all_text = soup.find_all(\"div\", {\"class\" : \"text_cell_render border-box-sizing rendered_html\"})\n",
    "        joined_text = \" \".join([txt.text for txt in all_text])\n",
    "        \n",
    "        # append lecture name and joined\n",
    "        # text to respective lists.\n",
    "        lecture_name = file.split(\"\\\\\")[1].split('.')[0]\n",
    "        lec_list.append(lecture_name)\n",
    "        joined_text_list.append(joined_text)\n",
    "        \n",
    "    \n",
    "    text_dict = {\"lec_name\" : lec_list, \"lec_text\" : joined_text_list}\n",
    "    \n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_texts = ipynb_html_text_parser(html_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocess text <a class=\"anchor\" id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Read in lecture notes text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lec_notes = pd.DataFrame(lecture_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lec_name</th>\n",
       "      <th>lec_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lecture3_floating-point</td>\n",
       "      <td>\\nDSCI 572 Lecture 3¶How to survive in a world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lecture5_neural-networks</td>\n",
       "      <td>\\nDSCI 572 \"lecture\" 5¶\\n \\nLecture outline:\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   lec_name                                           lec_text\n",
       "0   lecture3_floating-point  \\nDSCI 572 Lecture 3¶How to survive in a world...\n",
       "1  lecture5_neural-networks  \\nDSCI 572 \"lecture\" 5¶\\n \\nLecture outline:\\n..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lec_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Will use the doc_class dictionary below for classification purposes later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_doc_indices = {i: x for (i, x) in enumerate(df_lec_notes[\"lec_name\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'lecture3_floating-point', 1: 'lecture5_neural-networks'}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lecture3_floating-point': 'https://github.com/UBC-MDS/DSCI_572_sup-learn-2/blob/master/lectures/lecture3_floating-point.ipynb',\n",
       " 'lecture5_neural-networks': 'https://github.com/UBC-MDS/DSCI_572_sup-learn-2/blob/master/lectures/lecture5_neural-networks.ipynb'}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lecture_urls = {labeled_doc_indices[0]: \"https://github.com/UBC-MDS/DSCI_572_sup-learn-2/blob/master/lectures/lecture3_floating-point.ipynb\",\n",
    "               labeled_doc_indices[1]: \"https://github.com/UBC-MDS/DSCI_572_sup-learn-2/blob/master/lectures/lecture5_neural-networks.ipynb\"}\n",
    "lecture_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Read in transcribed notes text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_notes = glob.glob(\"data/text/transc*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/text\\\\transcribed_vid_1.txt', 'data/text\\\\transcribed_vid_2.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribed_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_texts = []\n",
    "vid_names = []\n",
    "for notes in transcribed_notes:\n",
    "    with open(notes, \"r\") as f:\n",
    "        transcribed_text = f.read()\n",
    "    vid_name = notes.split(\"\\\\\")[1].split(\".\")[0].split(\"ed_\")[1] # get video name\n",
    "    transcribed_texts.append(transcribed_text) # append video texts to list\n",
    "    vid_names.append(vid_name) # append video names to list \n",
    "transcribed_notes_dict = {\"vid_name\" : vid_names, \"transc_text\": transcribed_texts} # create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid_name</th>\n",
       "      <th>transc_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vid_1</td>\n",
       "      <td>let's get started are you good at the So today...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vid_2</td>\n",
       "      <td>all right let's go starting neural networks to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vid_name                                        transc_text\n",
       "0    vid_1  let's get started are you good at the So today...\n",
       "1    vid_2  all right let's go starting neural networks to..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transcr_notes = pd.DataFrame(transcribed_notes_dict)\n",
    "df_transcr_notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English model for SpaCy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is a faster way to parallelize this process, but I am on windows and multiprocessing causes issues. Moreover, the corpuses are quite small in this example, so there is no need to parallelize or even vectorize for that matter. \n",
    "\n",
    "> I will remove the following Part of Speech tags from the corpuses:\n",
    "- ADJ (Adjective) - not necessary in this context, as technical terms would be more important, adjectives can repeat across documents and increase the similarity.\n",
    "- CCONJ (Conjunction) - Can repeat across multiple documents.\n",
    "- PRON (Pronoun) - Can repeat across multiple documents.\n",
    "- PUNCT (Punctuation) - Not necessary, doesn't add meaning\n",
    "- PART (Possesive ending) - Not necessary, doesn't add meaning\n",
    "- DET (Predeterminer) - Can repeat across multiple documents.\n",
    "- ADP (Conjunction) - Can repeat across multiple documents.\n",
    "- SPACE (Whitespaces) - Gets rid of tabs, newlines, etc.\n",
    "- SYM (Symbol) - Could have been useful if there's a lot of math operations , however, converting speech to text and comparing these math operations with lecture notes math may not be the same depending on complexity.\n",
    "- SCONJ (Subordinate conjunction) - Can repeat across multiple documents.\n",
    "- INTJ (interjection) - No meaning in this case\n",
    "- VERB (verb) - Can repeat, no added meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_preprocess(token, min_token_len, irrelevant_pos):\n",
    "        if len(token.lemma_) < min_token_len:\n",
    "            pass\n",
    "        # if irrelevant pos, pass\n",
    "        elif token.pos_ in irrelevant_pos:\n",
    "            pass\n",
    "        # if email, pass\n",
    "        elif token.like_email:\n",
    "            pass\n",
    "        # if url, pass\n",
    "        elif token.like_url:\n",
    "            pass\n",
    "        # if stop word, pass\n",
    "        elif token.lemma_ in nlp.Defaults.stop_words:\n",
    "            pass\n",
    "        else:\n",
    "            # clean symbols within token\n",
    "            new_token = \"\"\n",
    "            for char in token.lemma_:\n",
    "                if char.isalnum():\n",
    "                    new_token+=char\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            return new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, \n",
    "               min_token_len = 2, \n",
    "               irrelevant_pos = ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE','SYM',\n",
    "                                'SCONJ', 'INTJ', 'VERB']): \n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text \n",
    "    and return a preprocessed string. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    text : (str) \n",
    "        the text to be preprocessed\n",
    "    min_token_len : (int) \n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list) \n",
    "        a list of irrelevant pos tags\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    df_words = pd.DataFrame({\"word\": doc})\n",
    "    \n",
    "    df_words[\"word_new\"] = df_words[\"word\"].apply(lambda x: vectorize_preprocess(x, \n",
    "                                                                                   min_token_len, \n",
    "                                                                                   irrelevant_pos))\n",
    "    \n",
    "    new_text = df_words[\"word_new\"].dropna().tolist()\n",
    "    new_text = \" \".join(new_text)\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lec_notes[\"prep_text\"] = df_lec_notes[\"lec_text\"].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transcr_notes[\"prep_text\"] = df_transcr_notes[\"transc_text\"].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Performing text similarity <a class=\"anchor\" id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Perform text similarity using TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is not the most effective way of computing document similarity, as it does not take into account the context. However, I wanted to try with the simplest model first to see if I get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_class_assign_tfidf(unlabeled_doc, labeled_docs):\n",
    "    \"\"\"\n",
    "    Computes and returns the\n",
    "    most similar index for\n",
    "    the unlabeled document\n",
    "    using TFIDF similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    unlabeled_doc: str\n",
    "        The document you wish\n",
    "        to classify.\n",
    "    \n",
    "    labeled_docs: str\n",
    "        The list, series of\n",
    "        labeled documents.\n",
    "        \n",
    "    Returns:\n",
    "    ---------\n",
    "    class assignment and the \n",
    "    similarity scores.    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> labeled_docs = [\"this is a big cat\",\n",
    "    ...                 \"unfair brown carpet\"]\n",
    "    >>> unseen_doc = \"how big is the cat\"\n",
    "    >>> doc_class_assign_tfidf(unseen_doc, labeled_docs)\n",
    "    {0: [0.8660254037844386, 0.0]}\n",
    "    \n",
    "    The unseen doc is most similar\n",
    "    to the 0th index of labeled docs\n",
    "    and it's respective similarity,\n",
    "    scores.\n",
    "    \"\"\"\n",
    "    # fit and transform tfidfvectorizer \n",
    "    # with labeled_docs and the unlabeled doc.\n",
    "    tfidf_model = TfidfVectorizer()\n",
    "    tfidf_labeled_docs = tfidf_model.fit_transform(labeled_docs)\n",
    "    tfidf_unlabeled_doc = tfidf_model.transform([unlabeled_doc])\n",
    "    \n",
    "    # get the similarity scores \n",
    "    # with respect to labeled documents\n",
    "    scores = [(tfidf_unlabeled_doc@x.T)[0,0] for x in tfidf_labeled_docs]\n",
    "    \n",
    "    # return index with max score\n",
    "    return {np.argmax(scores): scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_lec_class = df_transcr_notes[\"prep_text\"].apply(lambda x: doc_class_assign_tfidf(x, df_lec_notes[\"prep_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores seems to be doing just fine for this example, `lecture document 0` is more closer to `vid_1` and `lecture document 1` is closer to `vid_2`. No need to improve the model for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results <a class=\"anchor\" id=\"3.0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_transcr_notes.drop(columns=[\"transc_text\", \"prep_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lecture class to lecture name using `labeled_docs_indices` from 2.2.1\n",
    "df_results[\"assigned_lecture_name\"] = pd.Series(assigned_lec_class.keys()).apply(lambda x: labeled_doc_indices[x]) \n",
    "# get the lecture URLs\n",
    "df_results[\"assigned_lecture_url\"] = df_results[\"assigned_lecture_name\"].apply(lambda x: lecture_urls[x])\n",
    "# get the video file name using `videos_dict` from 1.1.1\n",
    "df_results[\"vid_file\"] = df_results[\"vid_name\"].apply(lambda x: videos_dict[x])\n",
    "# get the video urls\n",
    "df_results[\"vid_url\"] = df_results[\"vid_file\"].apply(lambda x: video_urls[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid_name</th>\n",
       "      <th>assigned_lecture_name</th>\n",
       "      <th>assigned_lecture_url</th>\n",
       "      <th>vid_file</th>\n",
       "      <th>vid_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vid_1</td>\n",
       "      <td>lecture3_floating-point</td>\n",
       "      <td>https://github.com/UBC-MDS/DSCI_572_sup-learn-...</td>\n",
       "      <td>20200120-102822-008.mp4</td>\n",
       "      <td>https://youtu.be/kNxLxwIduko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vid_2</td>\n",
       "      <td>lecture5_neural-networks</td>\n",
       "      <td>https://github.com/UBC-MDS/DSCI_572_sup-learn-...</td>\n",
       "      <td>20200127-102817-008.mp4</td>\n",
       "      <td>https://youtu.be/1wYHXT-Ee5U</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vid_name     assigned_lecture_name  \\\n",
       "0    vid_1   lecture3_floating-point   \n",
       "1    vid_2  lecture5_neural-networks   \n",
       "\n",
       "                                assigned_lecture_url                 vid_file  \\\n",
       "0  https://github.com/UBC-MDS/DSCI_572_sup-learn-...  20200120-102822-008.mp4   \n",
       "1  https://github.com/UBC-MDS/DSCI_572_sup-learn-...  20200127-102817-008.mp4   \n",
       "\n",
       "                        vid_url  \n",
       "0  https://youtu.be/kNxLxwIduko  \n",
       "1  https://youtu.be/1wYHXT-Ee5U  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Voilà! There we have it, successful document classification of unclassified videos. I can't share the results and analysis for all the lecture videos in my program as they are not publicly available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a class=\"anchor\" id=\"references\"></a>\n",
    "- [1] https://codeloop.org/python-how-to-convert-recorded-audio-to-text/\n",
    "- [2] https://www.geeksforgeeks.org/audio-processing-using-pydub-and-google-speechrecognition-api/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
